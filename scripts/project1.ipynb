{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from helpers.least_squares import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers.proj1_helpers import *\n",
    "# Aya : '/Users/mac/Documents/GitHub/ml-project-1-aaa_project1/data/train.csv' \n",
    "DATA_TRAIN_PATH = '/home/adeye/Documents/EPFL/MA1/ML_course/projects/ml-project-1-aaa_project1/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.gradient_descent import *\n",
    "from helpers.least_squares import *\n",
    "from helpers.stochastic_gradient_descent import *\n",
    "from helpers.ridge import *\n",
    "from helpers.processing import *\n",
    "from helpers.cross_validation import *\n",
    "from helpers.feature_transformation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing our DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_0,label_0,jet_1,label_1,jet_2_3, label_2_3= pre_process_data_pipeline(tX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms After Cleaning Our DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares With Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jet_0,label_0,jet_1,label_1,jet_2_3, label_2_3= groupy_by_jet_num(tX,y)\n",
    "    \n",
    "remove_outliers(tX)\n",
    "init_weights = np.ones((30,))\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "\n",
    "weights,loss =  gradient_descent(y, tX,init_weights , 50000, .01)\n",
    "\n",
    "print(weights)\n",
    "print(\"{:e}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares with OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "weight,loss= least_squares(y,tX)\n",
    "print(compute_squared_loss(y,tX,weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares With Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_outliers(tX)\n",
    "init_weights = np.ones((30,))\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "\n",
    "weights,loss =  stochastic_gradient_descent(y, tX,init_weights , 500, 0.01, batch_size=1)\n",
    "\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares With Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights, loss = ridge(y, tX, 0.025)\n",
    "print(\"{:e}\".format(np.sqrt(2*loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best hyperparameters for ridge regression (degree and lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 10 fold cross-validation for this assignment as it gives us a good compromise (for our hardware) between low bias and good performance\n",
    "k_fold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each one of our subsets we find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 0.0  Lambda: 1e-07  RMSE Training: 0.8718559628903682  RMSE Test: 0.8718523920593633\n",
      "Degree: 1.0  Lambda: 1e-07  RMSE Training: 0.7377063950311074  RMSE Test: 0.7380711683005949\n",
      "Degree: 2.0  Lambda: 1e-07  RMSE Training: 0.7130765956855225  RMSE Test: 1.3897213743532544\n",
      "Degree: 3.0  Lambda: 1e-07  RMSE Training: 0.7007905657168225  RMSE Test: 1.2374116524923102\n",
      "Degree: 4.0  Lambda: 1e-07  RMSE Training: 0.6959288240330819  RMSE Test: 2712.8479224220073\n",
      "Degree: 5.0  Lambda: 1e-07  RMSE Training: 0.9652245235136585  RMSE Test: 405271.98746283853\n",
      "Degree: 6.0  Lambda: 1e-07  RMSE Training: 5.939070955442334  RMSE Test: 13025329.090063512\n",
      "Degree: 7.0  Lambda: 1e-07  RMSE Training: 23.986333598086905  RMSE Test: 215004886.11787271\n",
      "Degree: 8.0  Lambda: 1e-07  RMSE Training: 2077404.7474475757  RMSE Test: 46359339282.487915\n",
      "Degree: 9.0  Lambda: 1e-07  RMSE Training: 13170744.523042297  RMSE Test: 16010273068497.037\n",
      "Degree: 0.0  Lambda: 1e-06  RMSE Training: 0.8718559628909128  RMSE Test: 0.8718523921049963\n",
      "Degree: 1.0  Lambda: 1e-06  RMSE Training: 0.7377556868103861  RMSE Test: 0.7380797320185006\n",
      "Degree: 2.0  Lambda: 1e-06  RMSE Training: 0.7146690714871415  RMSE Test: 1.3981186103910705\n",
      "Degree: 3.0  Lambda: 1e-06  RMSE Training: 0.7021847008227318  RMSE Test: 0.8807799961328993\n",
      "Degree: 4.0  Lambda: 1e-06  RMSE Training: 0.6972595251416129  RMSE Test: 2674.000445852468\n",
      "Degree: 5.0  Lambda: 1e-06  RMSE Training: 1.1625583951954528  RMSE Test: 404219.08315037104\n",
      "Degree: 6.0  Lambda: 1e-06  RMSE Training: 3.2255991058710527  RMSE Test: 12941423.351856505\n",
      "Degree: 7.0  Lambda: 1e-06  RMSE Training: 37.27779622043617  RMSE Test: 197964308.70137113\n",
      "Degree: 8.0  Lambda: 1e-06  RMSE Training: 130438.96318966438  RMSE Test: 42765595191.87294\n",
      "Degree: 9.0  Lambda: 1e-06  RMSE Training: 10266807.562242221  RMSE Test: 7717914641544.886\n",
      "Degree: 0.0  Lambda: 1e-05  RMSE Training: 0.871855962945385  RMSE Test: 0.8718523926103356\n",
      "Degree: 1.0  Lambda: 1e-05  RMSE Training: 0.7400726579070419  RMSE Test: 0.740343403068716\n",
      "Degree: 2.0  Lambda: 1e-05  RMSE Training: 0.7157161388104791  RMSE Test: 1.3859666703159\n",
      "Degree: 3.0  Lambda: 1e-05  RMSE Training: 0.703522730445858  RMSE Test: 0.8528542786258656\n",
      "Degree: 4.0  Lambda: 1e-05  RMSE Training: 0.6989284816022974  RMSE Test: 2744.5105677721567\n",
      "Degree: 5.0  Lambda: 1e-05  RMSE Training: 0.7020395205399305  RMSE Test: 384813.03694775014\n",
      "Degree: 6.0  Lambda: 1e-05  RMSE Training: 6.012635750798544  RMSE Test: 10586849.484243847\n",
      "Degree: 7.0  Lambda: 1e-05  RMSE Training: 36.43781179844011  RMSE Test: 185761057.17383862\n",
      "Degree: 8.0  Lambda: 1e-05  RMSE Training: 72380.25962763776  RMSE Test: 34138362714.73913\n",
      "Degree: 9.0  Lambda: 1e-05  RMSE Training: 3369112.219469386  RMSE Test: 1554416275265.062\n",
      "Degree: 0.0  Lambda: 0.0001  RMSE Training: 0.8718559683906232  RMSE Test: 0.8718524025626436\n",
      "Degree: 1.0  Lambda: 0.0001  RMSE Training: 0.7545143520739057  RMSE Test: 0.7552610909327655\n",
      "Degree: 2.0  Lambda: 0.0001  RMSE Training: 0.7244188073220144  RMSE Test: 1.3900041616478223\n",
      "Degree: 3.0  Lambda: 0.0001  RMSE Training: 0.7086243380917441  RMSE Test: 1.7154449162919843\n",
      "Degree: 4.0  Lambda: 0.0001  RMSE Training: 0.7028800227149925  RMSE Test: 2738.8161681537213\n",
      "Degree: 5.0  Lambda: 0.0001  RMSE Training: 0.7348365898861555  RMSE Test: 274756.5705852627\n",
      "Degree: 6.0  Lambda: 0.0001  RMSE Training: 11.102442172000881  RMSE Test: 3460769.7704363847\n",
      "Degree: 7.0  Lambda: 0.0001  RMSE Training: 39.57598117663949  RMSE Test: 314180696.4328067\n",
      "Degree: 8.0  Lambda: 0.0001  RMSE Training: 22622.47313759566  RMSE Test: 21431323707.02868\n",
      "Degree: 9.0  Lambda: 0.0001  RMSE Training: 5108060.855799853  RMSE Test: 433572883884.8287\n",
      "Degree: 0.0  Lambda: 0.001  RMSE Training: 0.8718565109418863  RMSE Test: 0.8718529900249182\n",
      "Degree: 1.0  Lambda: 0.001  RMSE Training: 0.7627633918454338  RMSE Test: 0.7636314193035635\n",
      "Degree: 2.0  Lambda: 0.001  RMSE Training: 0.7329765333274831  RMSE Test: 1.3701443600953636\n",
      "Degree: 3.0  Lambda: 0.001  RMSE Training: 0.7150441873623612  RMSE Test: 0.9377900399950825\n",
      "Degree: 4.0  Lambda: 0.001  RMSE Training: 0.7083766818236659  RMSE Test: 1271.7657095945285\n",
      "Degree: 5.0  Lambda: 0.001  RMSE Training: 0.7051152254656217  RMSE Test: 70274.36192260207\n",
      "Degree: 6.0  Lambda: 0.001  RMSE Training: 5.379273440059019  RMSE Test: 1354886.4824854909\n",
      "Degree: 7.0  Lambda: 0.001  RMSE Training: 46.12809493858807  RMSE Test: 198596424.1314244\n",
      "Degree: 8.0  Lambda: 0.001  RMSE Training: 30077041.899233818  RMSE Test: 3293386261.537834\n",
      "Degree: 9.0  Lambda: 0.001  RMSE Training: 107276512.35111073  RMSE Test: 162934161076.00687\n",
      "Degree: 0.0  Lambda: 0.01  RMSE Training: 0.8719088492231185  RMSE Test: 0.8719057618187719\n",
      "Degree: 1.0  Lambda: 0.01  RMSE Training: 0.7648787559024013  RMSE Test: 0.765776628056906\n",
      "Degree: 2.0  Lambda: 0.01  RMSE Training: 0.7365021821860676  RMSE Test: 1.30051216084513\n",
      "Degree: 3.0  Lambda: 0.01  RMSE Training: 0.7200849210690166  RMSE Test: 7.650962285938486\n",
      "Degree: 4.0  Lambda: 0.01  RMSE Training: 0.7158482074306796  RMSE Test: 136.11720375576155\n",
      "Degree: 5.0  Lambda: 0.01  RMSE Training: 0.7130683295238468  RMSE Test: 13316.512656650617\n",
      "Degree: 6.0  Lambda: 0.01  RMSE Training: 6.615808712458782  RMSE Test: 84457.1357417111\n",
      "Degree: 7.0  Lambda: 0.01  RMSE Training: 275.87622073894204  RMSE Test: 29719019.05815202\n",
      "Degree: 8.0  Lambda: 0.01  RMSE Training: 308825.25718462555  RMSE Test: 459494644.17467374\n",
      "Degree: 9.0  Lambda: 0.01  RMSE Training: 36038123.55208169  RMSE Test: 33616625356.052013\n",
      "Degree: 0.0  Lambda: 0.1  RMSE Training: 0.8756687789072076  RMSE Test: 0.8756687842029093\n",
      "Degree: 1.0  Lambda: 0.1  RMSE Training: 0.7797648526979413  RMSE Test: 0.780037895295508\n",
      "Degree: 2.0  Lambda: 0.1  RMSE Training: 0.761089485278178  RMSE Test: 1.035826499979908\n",
      "Degree: 3.0  Lambda: 0.1  RMSE Training: 0.7562812237246049  RMSE Test: 1.838437842387711\n",
      "Degree: 4.0  Lambda: 0.1  RMSE Training: 0.7532888649935432  RMSE Test: 36.085431870120566\n",
      "Degree: 5.0  Lambda: 0.1  RMSE Training: 0.7486119533914009  RMSE Test: 1561.0916865972906\n",
      "Degree: 6.0  Lambda: 0.1  RMSE Training: 54.64040058267293  RMSE Test: 241971.17939191955\n",
      "Degree: 7.0  Lambda: 0.1  RMSE Training: 572.4291606001369  RMSE Test: 13853926.907827025\n",
      "Degree: 8.0  Lambda: 0.1  RMSE Training: 152525.3945283476  RMSE Test: 356935680.63415426\n",
      "Degree: 9.0  Lambda: 0.1  RMSE Training: 29017156.61818265  RMSE Test: 16734060957.282812\n",
      "Degree: 0.0  Lambda: 1.0  RMSE Training: 0.9309890233419313  RMSE Test: 0.9309919553361731\n",
      "Degree: 1.0  Lambda: 1.0  RMSE Training: 0.8451169601106765  RMSE Test: 0.8451272990595108\n",
      "Degree: 2.0  Lambda: 1.0  RMSE Training: 0.8387211168316041  RMSE Test: 0.8400946198308628\n",
      "Degree: 3.0  Lambda: 1.0  RMSE Training: 0.8342937035575841  RMSE Test: 4.282784188805329\n",
      "Degree: 4.0  Lambda: 1.0  RMSE Training: 0.8333671319277958  RMSE Test: 44.50194549749947\n",
      "Degree: 5.0  Lambda: 1.0  RMSE Training: 0.8278696607301382  RMSE Test: 1995.7713982236878\n",
      "Degree: 6.0  Lambda: 1.0  RMSE Training: 0.82450631698341  RMSE Test: 82281.7856232042\n",
      "Degree: 7.0  Lambda: 1.0  RMSE Training: 767.0249711908833  RMSE Test: 9464013.986819398\n",
      "Degree: 8.0  Lambda: 1.0  RMSE Training: 36136032.276853554  RMSE Test: 179191846.16347498\n",
      "Degree: 9.0  Lambda: 1.0  RMSE Training: 23427638.867854636  RMSE Test: 20879622135.26075\n",
      "\n",
      "\n",
      "\n",
      "*******JET 0 *************\n",
      "Lowest_error: 0.7380711683005949  Best degree: 1.0  Best lambda: 1e-07\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "#For jet_0 => find the best hyperparameters and find the weight that corresponds to those hyperparameters\n",
    "\n",
    "[lowest_error_0,best_degree_0,best_lambda_0,rmse_tr,rmse_te] = cross_validation_ridge(k_fold,jet_0,label_0)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"*******JET 0 *************\")\n",
    "print(\"Lowest_error:\",lowest_error_0,\" Best degree:\",best_degree_0,\" Best lambda:\",best_lambda_0)\n",
    "    \n",
    "\n",
    "#X,Y = np.meshgrid(rmse_tr[:,0],rmse_tr[:,1])\n",
    "#print(X)\n",
    "#print(Y)\n",
    "\n",
    "#print(rmse_tr[:,2])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(projection='3d')\n",
    "#ax.scatter(rmse_tr[:,0],rmse_tr[:,1],rmse_tr[:,2])\n",
    "#plt.xlim([0, 1e+08])\n",
    "\n",
    "#plt.show()\n",
    "jet_0_extended = poly_expansion(jet_0,best_degree_0)\n",
    "w_jet_0,_ = ridge(label_0,jet_0_extended,best_lambda_0)\n",
    "\n",
    "print(w_jet_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 0.0  Lambda: 1e-07  RMSE Training: 0.9584450111432654  RMSE Test: 0.958463428207593\n",
      "Degree: 1.0  Lambda: 1e-07  RMSE Training: 0.8632964889818588  RMSE Test: 0.8635846708506504\n",
      "Degree: 2.0  Lambda: 1e-07  RMSE Training: 0.8334061642407902  RMSE Test: 0.8345179074773321\n",
      "Degree: 3.0  Lambda: 1e-07  RMSE Training: 0.81624333928636  RMSE Test: 0.8219852518359223\n",
      "Degree: 4.0  Lambda: 1e-07  RMSE Training: 0.806101215946956  RMSE Test: 0.8222089435885938\n",
      "Degree: 5.0  Lambda: 1e-07  RMSE Training: 0.8008871275776878  RMSE Test: 0.8071947829219605\n",
      "Degree: 6.0  Lambda: 1e-07  RMSE Training: 0.7946266156834191  RMSE Test: 0.8437188420491293\n",
      "Degree: 7.0  Lambda: 1e-07  RMSE Training: 0.7848271875101357  RMSE Test: 1.2580566848524606\n",
      "Degree: 8.0  Lambda: 1e-07  RMSE Training: 0.7749472172926022  RMSE Test: 2.3264335979806874\n",
      "Degree: 9.0  Lambda: 1e-07  RMSE Training: 0.7706629314416529  RMSE Test: 4.408002340027459\n",
      "Degree: 0.0  Lambda: 1e-06  RMSE Training: 0.9584450111434336  RMSE Test: 0.9584634281973902\n",
      "Degree: 1.0  Lambda: 1e-06  RMSE Training: 0.8633400817685235  RMSE Test: 0.8636259129623847\n",
      "Degree: 2.0  Lambda: 1e-06  RMSE Training: 0.8442586237952249  RMSE Test: 0.8453547319595659\n",
      "Degree: 3.0  Lambda: 1e-06  RMSE Training: 0.8244223639073504  RMSE Test: 0.8302683462789007\n",
      "Degree: 4.0  Lambda: 1e-06  RMSE Training: 0.8106344251401  RMSE Test: 0.8267311696479375\n",
      "Degree: 5.0  Lambda: 1e-06  RMSE Training: 0.8036462202321136  RMSE Test: 0.8096582453601135\n",
      "Degree: 6.0  Lambda: 1e-06  RMSE Training: 0.7964597208792863  RMSE Test: 0.8434549399593761\n",
      "Degree: 7.0  Lambda: 1e-06  RMSE Training: 0.7861913502821523  RMSE Test: 1.2597479103766822\n",
      "Degree: 8.0  Lambda: 1e-06  RMSE Training: 0.7761175289594007  RMSE Test: 2.3194687575878867\n",
      "Degree: 9.0  Lambda: 1e-06  RMSE Training: 0.7717376725016865  RMSE Test: 4.236231292787396\n",
      "Degree: 0.0  Lambda: 1e-05  RMSE Training: 0.9584450111602456  RMSE Test: 0.9584634281104887\n",
      "Degree: 1.0  Lambda: 1e-05  RMSE Training: 0.8652206551284121  RMSE Test: 0.8654936987523726\n",
      "Degree: 2.0  Lambda: 1e-05  RMSE Training: 0.8481747128737332  RMSE Test: 0.8492289828796645\n",
      "Degree: 3.0  Lambda: 1e-05  RMSE Training: 0.8324832411340116  RMSE Test: 0.8383087679548205\n",
      "Degree: 4.0  Lambda: 1e-05  RMSE Training: 0.8202901329816894  RMSE Test: 0.8361497121914319\n",
      "Degree: 5.0  Lambda: 1e-05  RMSE Training: 0.8130817146844633  RMSE Test: 0.8193135972752404\n",
      "Degree: 6.0  Lambda: 1e-05  RMSE Training: 0.8049701531592433  RMSE Test: 0.852161123104799\n",
      "Degree: 7.0  Lambda: 1e-05  RMSE Training: 0.7936055206230211  RMSE Test: 1.2853346672012016\n",
      "Degree: 8.0  Lambda: 1e-05  RMSE Training: 0.782698811824746  RMSE Test: 2.253031716167297\n",
      "Degree: 9.0  Lambda: 1e-05  RMSE Training: 0.7778715868466075  RMSE Test: 2.858848704916224\n",
      "Degree: 0.0  Lambda: 0.0001  RMSE Training: 0.9584450128408234  RMSE Test: 0.9584634287536007\n",
      "Degree: 1.0  Lambda: 0.0001  RMSE Training: 0.8753901523627198  RMSE Test: 0.8756342339380163\n",
      "Degree: 2.0  Lambda: 0.0001  RMSE Training: 0.8539579164252895  RMSE Test: 0.8549742321713486\n",
      "Degree: 3.0  Lambda: 0.0001  RMSE Training: 0.8366112786359107  RMSE Test: 0.842538019240561\n",
      "Degree: 4.0  Lambda: 0.0001  RMSE Training: 0.824479962276478  RMSE Test: 0.8402662463732631\n",
      "Degree: 5.0  Lambda: 0.0001  RMSE Training: 0.818223552474201  RMSE Test: 0.8252911614196673\n",
      "Degree: 6.0  Lambda: 0.0001  RMSE Training: 0.8110590937898856  RMSE Test: 0.861142069968403\n",
      "Degree: 7.0  Lambda: 0.0001  RMSE Training: 0.800445720859759  RMSE Test: 1.3171378469860462\n",
      "Degree: 8.0  Lambda: 0.0001  RMSE Training: 0.7904693684830715  RMSE Test: 2.3178441583505687\n",
      "Degree: 9.0  Lambda: 0.0001  RMSE Training: 0.7861435086747087  RMSE Test: 1.763625270856938\n",
      "Degree: 0.0  Lambda: 0.001  RMSE Training: 0.958445180289865  RMSE Test: 0.958463585794936\n",
      "Degree: 1.0  Lambda: 0.001  RMSE Training: 0.8844222352651361  RMSE Test: 0.8846138507900289\n",
      "Degree: 2.0  Lambda: 0.001  RMSE Training: 0.8606241570856001  RMSE Test: 0.8615931757199105\n",
      "Degree: 3.0  Lambda: 0.001  RMSE Training: 0.8411293735508154  RMSE Test: 0.8469162215294481\n",
      "Degree: 4.0  Lambda: 0.001  RMSE Training: 0.8277332754786239  RMSE Test: 0.8416887690018442\n",
      "Degree: 5.0  Lambda: 0.001  RMSE Training: 0.8210523506036667  RMSE Test: 0.8283418699566294\n",
      "Degree: 6.0  Lambda: 0.001  RMSE Training: 0.8147284436817097  RMSE Test: 0.8971241549986864\n",
      "Degree: 7.0  Lambda: 0.001  RMSE Training: 0.8073579133663882  RMSE Test: 1.3621257225761334\n",
      "Degree: 8.0  Lambda: 0.001  RMSE Training: 0.7995660960336947  RMSE Test: 1.7371691763607697\n",
      "Degree: 9.0  Lambda: 0.001  RMSE Training: 0.7964511276004848  RMSE Test: 2.496091206817428\n",
      "Degree: 0.0  Lambda: 0.01  RMSE Training: 0.9584613339453648  RMSE Test: 0.9584796322418759\n",
      "Degree: 1.0  Lambda: 0.01  RMSE Training: 0.887741064520657  RMSE Test: 0.8878813094263187\n",
      "Degree: 2.0  Lambda: 0.01  RMSE Training: 0.8654195960316329  RMSE Test: 0.8661389331287787\n",
      "Degree: 3.0  Lambda: 0.01  RMSE Training: 0.8479944881603994  RMSE Test: 0.8522985062126439\n",
      "Degree: 4.0  Lambda: 0.01  RMSE Training: 0.8361702124914995  RMSE Test: 0.8421213748075459\n",
      "Degree: 5.0  Lambda: 0.01  RMSE Training: 0.831585827997109  RMSE Test: 0.8495163918977495\n",
      "Degree: 6.0  Lambda: 0.01  RMSE Training: 0.8273635771578409  RMSE Test: 0.980381510009661\n",
      "Degree: 7.0  Lambda: 0.01  RMSE Training: 0.8193179740820726  RMSE Test: 1.0987194598584302\n",
      "Degree: 8.0  Lambda: 0.01  RMSE Training: 0.8151615494830893  RMSE Test: 1.307650632223492\n",
      "Degree: 9.0  Lambda: 0.01  RMSE Training: 0.8132091363911755  RMSE Test: 4.2771409997167416\n",
      "Degree: 0.0  Lambda: 0.1  RMSE Training: 0.9596236188515321  RMSE Test: 0.9596406614571483\n",
      "Degree: 1.0  Lambda: 0.1  RMSE Training: 0.9031203485607305  RMSE Test: 0.9031907044613634\n",
      "Degree: 2.0  Lambda: 0.1  RMSE Training: 0.8885116884895641  RMSE Test: 0.8889064070293724\n",
      "Degree: 3.0  Lambda: 0.1  RMSE Training: 0.8788955254941466  RMSE Test: 0.8799176144091823\n",
      "Degree: 4.0  Lambda: 0.1  RMSE Training: 0.8754653110089012  RMSE Test: 0.8800026610127946\n",
      "Degree: 5.0  Lambda: 0.1  RMSE Training: 0.8707414029984873  RMSE Test: 0.9096314346439163\n",
      "Degree: 6.0  Lambda: 0.1  RMSE Training: 0.8618543228130962  RMSE Test: 0.9296516657717909\n",
      "Degree: 7.0  Lambda: 0.1  RMSE Training: 0.8590107380803609  RMSE Test: 1.0347199214812188\n",
      "Degree: 8.0  Lambda: 0.1  RMSE Training: 0.8557873727753439  RMSE Test: 1.804063033905749\n",
      "Degree: 9.0  Lambda: 0.1  RMSE Training: 0.8477622105425752  RMSE Test: 3.216351178259006\n",
      "Degree: 0.0  Lambda: 1.0  RMSE Training: 0.9771321281128118  RMSE Test: 0.9771405773011912\n",
      "Degree: 1.0  Lambda: 1.0  RMSE Training: 0.9431400665068095  RMSE Test: 0.943171485544901\n",
      "Degree: 2.0  Lambda: 1.0  RMSE Training: 0.9366756712760156  RMSE Test: 0.9368246758716687\n",
      "Degree: 3.0  Lambda: 1.0  RMSE Training: 0.9333106964004261  RMSE Test: 0.9335052320566544\n",
      "Degree: 4.0  Lambda: 1.0  RMSE Training: 0.93159321852218  RMSE Test: 0.935097812098509\n",
      "Degree: 5.0  Lambda: 1.0  RMSE Training: 0.9269770496024995  RMSE Test: 0.9425313643676118\n",
      "Degree: 6.0  Lambda: 1.0  RMSE Training: 0.92443075415232  RMSE Test: 0.9510147119584019\n",
      "Degree: 7.0  Lambda: 1.0  RMSE Training: 0.922170436557988  RMSE Test: 1.1610520454117694\n",
      "Degree: 8.0  Lambda: 1.0  RMSE Training: 0.9186393745325923  RMSE Test: 1.4355646305428647\n",
      "Degree: 9.0  Lambda: 1.0  RMSE Training: 0.9167312364794002  RMSE Test: 2.1694664234108174\n",
      "\n",
      "\n",
      "\n",
      "*******JET 1 *************\n",
      "Lowest_error: 0.8071947829219605  Best degree: 5.0  Best lambda: 1e-07\n",
      "(111,)\n"
     ]
    }
   ],
   "source": [
    "#For jet_1 => find the best hyperparameters and find the weight that corresponds to those hyperparameters\n",
    "\n",
    "[lowest_error_1,best_degree_1,best_lambda_1,rmse_tr,rmse_te] = cross_validation_ridge(k_fold,jet_1,label_1)\n",
    "print(\"\\n\\n\")\n",
    "print(\"*******JET 1 *************\")\n",
    "print(\"Lowest_error:\",lowest_error_1,\" Best degree:\",best_degree_1,\" Best lambda:\",best_lambda_1)\n",
    "\n",
    "jet_1_extended = poly_expansion(jet_1,best_degree_1)\n",
    "w_jet_1,_ = ridge(label_1,jet_1_extended,best_lambda_1)\n",
    "\n",
    "print(w_jet_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 0.0  Lambda: 1e-07  RMSE Training: 0.9944759461700402  RMSE Test: 0.9944798175507181\n",
      "Degree: 1.0  Lambda: 1e-07  RMSE Training: 0.8498488074937182  RMSE Test: 0.8502854000440656\n",
      "Degree: 2.0  Lambda: 1e-07  RMSE Training: 0.8316080381404045  RMSE Test: 0.8324954708330303\n",
      "Degree: 3.0  Lambda: 1e-07  RMSE Training: 0.8068073066289368  RMSE Test: 0.8090088856985981\n",
      "Degree: 4.0  Lambda: 1e-07  RMSE Training: 0.7890532717947665  RMSE Test: 0.7970387612984328\n",
      "Degree: 5.0  Lambda: 1e-07  RMSE Training: 0.7841368653010241  RMSE Test: 0.801666559037433\n",
      "Degree: 6.0  Lambda: 1e-07  RMSE Training: 0.7774009662002904  RMSE Test: 1.0014903471087893\n",
      "Degree: 7.0  Lambda: 1e-07  RMSE Training: 0.7615315050984078  RMSE Test: 0.9344830921399175\n",
      "Degree: 8.0  Lambda: 1e-07  RMSE Training: 0.7452866383354437  RMSE Test: 1.6413923221003077\n",
      "Degree: 9.0  Lambda: 1e-07  RMSE Training: 0.738624463295954  RMSE Test: 6.96370101675611\n",
      "Degree: 0.0  Lambda: 1e-06  RMSE Training: 0.9944759461700621  RMSE Test: 0.9944798175441785\n",
      "Degree: 1.0  Lambda: 1e-06  RMSE Training: 0.8511315013393643  RMSE Test: 0.8515423873820225\n",
      "Degree: 2.0  Lambda: 1e-06  RMSE Training: 0.8331481586693104  RMSE Test: 0.8339962826404523\n",
      "Degree: 3.0  Lambda: 1e-06  RMSE Training: 0.8081825481649425  RMSE Test: 0.8103272240329609\n",
      "Degree: 4.0  Lambda: 1e-06  RMSE Training: 0.7904239142207394  RMSE Test: 0.7983948054636378\n",
      "Degree: 5.0  Lambda: 1e-06  RMSE Training: 0.7855765707921918  RMSE Test: 0.8029071362718285\n",
      "Degree: 6.0  Lambda: 1e-06  RMSE Training: 0.7788373907297862  RMSE Test: 0.9979315842057737\n",
      "Degree: 7.0  Lambda: 1e-06  RMSE Training: 0.7629331046434402  RMSE Test: 0.9101054597370071\n",
      "Degree: 8.0  Lambda: 1e-06  RMSE Training: 0.7467179494525009  RMSE Test: 1.5898312466454108\n",
      "Degree: 9.0  Lambda: 1e-06  RMSE Training: 0.7403175406733985  RMSE Test: 5.371271251258433\n",
      "Degree: 0.0  Lambda: 1e-05  RMSE Training: 0.9944759461722557  RMSE Test: 0.9944798174807576\n",
      "Degree: 1.0  Lambda: 1e-05  RMSE Training: 0.8621936082078321  RMSE Test: 0.8625442945319992\n",
      "Degree: 2.0  Lambda: 1e-05  RMSE Training: 0.8392617304286285  RMSE Test: 0.8400587106739223\n",
      "Degree: 3.0  Lambda: 1e-05  RMSE Training: 0.8120448190261165  RMSE Test: 0.8141500714228889\n",
      "Degree: 4.0  Lambda: 1e-05  RMSE Training: 0.7937432229033149  RMSE Test: 0.8020504256641618\n",
      "Degree: 5.0  Lambda: 1e-05  RMSE Training: 0.7889358944400695  RMSE Test: 0.8066239343391203\n",
      "Degree: 6.0  Lambda: 1e-05  RMSE Training: 0.7822451751396807  RMSE Test: 0.980142716628615\n",
      "Degree: 7.0  Lambda: 1e-05  RMSE Training: 0.7664740038352149  RMSE Test: 0.9296919224913935\n",
      "Degree: 8.0  Lambda: 1e-05  RMSE Training: 0.7523881750021764  RMSE Test: 1.1168109513243867\n",
      "Degree: 9.0  Lambda: 1e-05  RMSE Training: 0.7512629498679384  RMSE Test: 3.7555487552390536\n",
      "Degree: 0.0  Lambda: 0.0001  RMSE Training: 0.9944759463915271  RMSE Test: 0.9944798170440036\n",
      "Degree: 1.0  Lambda: 0.0001  RMSE Training: 0.8740115014393421  RMSE Test: 0.8742884847151625\n",
      "Degree: 2.0  Lambda: 0.0001  RMSE Training: 0.8466398239417543  RMSE Test: 0.8473726557604975\n",
      "Degree: 3.0  Lambda: 0.0001  RMSE Training: 0.8172254443850349  RMSE Test: 0.8193406759454028\n",
      "Degree: 4.0  Lambda: 0.0001  RMSE Training: 0.7984443939772882  RMSE Test: 0.8070824627156545\n",
      "Degree: 5.0  Lambda: 0.0001  RMSE Training: 0.7939827481629138  RMSE Test: 0.8124244440015989\n",
      "Degree: 6.0  Lambda: 0.0001  RMSE Training: 0.787439517849411  RMSE Test: 0.9232729396203412\n",
      "Degree: 7.0  Lambda: 0.0001  RMSE Training: 0.7755306866739466  RMSE Test: 0.9702351342498581\n",
      "Degree: 8.0  Lambda: 0.0001  RMSE Training: 0.7733979761728242  RMSE Test: 1.1828376432297258\n",
      "Degree: 9.0  Lambda: 0.0001  RMSE Training: 0.7707429739320533  RMSE Test: 2.9757429862791454\n",
      "Degree: 0.0  Lambda: 0.001  RMSE Training: 0.9944759682392682  RMSE Test: 0.9944798323434192\n",
      "Degree: 1.0  Lambda: 0.001  RMSE Training: 0.8779756872745341  RMSE Test: 0.8781828735024098\n",
      "Degree: 2.0  Lambda: 0.001  RMSE Training: 0.8494303255266132  RMSE Test: 0.8499979523067976\n",
      "Degree: 3.0  Lambda: 0.001  RMSE Training: 0.820995915777113  RMSE Test: 0.8226918644532377\n",
      "Degree: 4.0  Lambda: 0.001  RMSE Training: 0.805838722445211  RMSE Test: 0.8130425788717666\n",
      "Degree: 5.0  Lambda: 0.001  RMSE Training: 0.8033412414547187  RMSE Test: 0.8113206621768432\n",
      "Degree: 6.0  Lambda: 0.001  RMSE Training: 0.7945894390119286  RMSE Test: 0.8144382704486883\n",
      "Degree: 7.0  Lambda: 0.001  RMSE Training: 0.7908453704215468  RMSE Test: 1.0114144224112436\n",
      "Degree: 8.0  Lambda: 0.001  RMSE Training: 0.7893248426758379  RMSE Test: 1.5038847432111246\n",
      "Degree: 9.0  Lambda: 0.001  RMSE Training: 0.7838428891056844  RMSE Test: 1.7913914965930549\n",
      "Degree: 0.0  Lambda: 0.01  RMSE Training: 0.9944780758865026  RMSE Test: 0.9944818756796124\n",
      "Degree: 1.0  Lambda: 0.01  RMSE Training: 0.8822713514362623  RMSE Test: 0.8824012004128431\n",
      "Degree: 2.0  Lambda: 0.01  RMSE Training: 0.8569339060200779  RMSE Test: 0.8573120163603314\n",
      "Degree: 3.0  Lambda: 0.01  RMSE Training: 0.8394365731413469  RMSE Test: 0.8405697999352691\n",
      "Degree: 4.0  Lambda: 0.01  RMSE Training: 0.8368802693653177  RMSE Test: 0.8385163495814474\n",
      "Degree: 5.0  Lambda: 0.01  RMSE Training: 0.8300626823290109  RMSE Test: 0.8343924200069587\n",
      "Degree: 6.0  Lambda: 0.01  RMSE Training: 0.8255435455343184  RMSE Test: 0.853214403607508\n",
      "Degree: 7.0  Lambda: 0.01  RMSE Training: 0.8236311707994594  RMSE Test: 0.9276580547675799\n",
      "Degree: 8.0  Lambda: 0.01  RMSE Training: 0.8182960425041257  RMSE Test: 0.9044357006767012\n",
      "Degree: 9.0  Lambda: 0.01  RMSE Training: 0.8187764935918496  RMSE Test: 1.2847089684280557\n",
      "Degree: 0.0  Lambda: 0.1  RMSE Training: 0.9946298064452236  RMSE Test: 0.9946330624810145\n",
      "Degree: 1.0  Lambda: 0.1  RMSE Training: 0.9052076786586858  RMSE Test: 0.9052655160459004\n",
      "Degree: 2.0  Lambda: 0.1  RMSE Training: 0.8912828244910124  RMSE Test: 0.8914752455507641\n",
      "Degree: 3.0  Lambda: 0.1  RMSE Training: 0.8882732698745173  RMSE Test: 0.8886208538314575\n",
      "Degree: 4.0  Lambda: 0.1  RMSE Training: 0.8846349001444486  RMSE Test: 0.8858314257409281\n",
      "Degree: 5.0  Lambda: 0.1  RMSE Training: 0.8810187733233112  RMSE Test: 0.8856332051850859\n",
      "Degree: 6.0  Lambda: 0.1  RMSE Training: 0.878654088740659  RMSE Test: 0.8878671539792362\n",
      "Degree: 7.0  Lambda: 0.1  RMSE Training: 0.875982522794328  RMSE Test: 0.9267396040817315\n",
      "Degree: 8.0  Lambda: 0.1  RMSE Training: 0.875923566025295  RMSE Test: 1.6485845274364297\n",
      "Degree: 9.0  Lambda: 0.1  RMSE Training: 0.8732953596436458  RMSE Test: 1.7061384050366502\n",
      "Degree: 0.0  Lambda: 1.0  RMSE Training: 0.9969348606604213  RMSE Test: 0.9969361914555723\n",
      "Degree: 1.0  Lambda: 1.0  RMSE Training: 0.9370326535511369  RMSE Test: 0.9370554736218747\n",
      "Degree: 2.0  Lambda: 1.0  RMSE Training: 0.9306818427495669  RMSE Test: 0.930754347967158\n",
      "Degree: 3.0  Lambda: 1.0  RMSE Training: 0.9256365603772989  RMSE Test: 0.9261693892720422\n",
      "Degree: 4.0  Lambda: 1.0  RMSE Training: 0.9173191888158598  RMSE Test: 0.9184617432034985\n",
      "Degree: 5.0  Lambda: 1.0  RMSE Training: 0.9138570251935543  RMSE Test: 0.9203551648430643\n",
      "Degree: 6.0  Lambda: 1.0  RMSE Training: 0.9120940402116968  RMSE Test: 0.9163842115880929\n",
      "Degree: 7.0  Lambda: 1.0  RMSE Training: 0.9115513142832784  RMSE Test: 1.03477518892105\n",
      "Degree: 8.0  Lambda: 1.0  RMSE Training: 0.9095741928239767  RMSE Test: 1.0079951946518924\n",
      "Degree: 9.0  Lambda: 1.0  RMSE Training: 0.9080822043943501  RMSE Test: 3.5275184300492137\n",
      "\n",
      "\n",
      "\n",
      "*******JET 2_3 *************\n",
      "Lowest_error: 0.7970387612984328  Best degree: 4.0  Best lambda: 1e-07\n",
      "(121,)\n"
     ]
    }
   ],
   "source": [
    "#For jet_2_3 => find the best hyperparameters and find the weight that corresponds to those hyperparameters\n",
    "\n",
    "[lowest_error_2_3,best_degree_2_3,best_lambda_2_3,rmse_tr,rmse_te] = cross_validation_ridge(k_fold,jet_2_3,label_2_3)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"*******JET 2_3 *************\")\n",
    "print(\"Lowest_error:\",lowest_error_2_3,\" Best degree:\",best_degree_2_3,\" Best lambda:\",best_lambda_2_3)\n",
    "\n",
    "jet_2_3_extended = poly_expansion(jet_2_3,best_degree_2_3)\n",
    "w_jet_2_3,_ = ridge(label_2_3,jet_2_3_extended,best_lambda_2_3)\n",
    "\n",
    "print(w_jet_2_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24330405912727762\n"
     ]
    }
   ],
   "source": [
    "#cross_validation_log_visualization(degrees,rmse_tr,rmse_te)\n",
    "#print(rmse_tr)\n",
    "#print(rmse_te)\n",
    "\n",
    "degree = 5\n",
    "lambda_ = 1e-07\n",
    "poly = poly_expansion(jet_0,degree)\n",
    "weight,loss = ridge(label_0, poly, lambda_)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '/home/adeye/Documents/EPFL/MA1/ML_course/projects/ml-project-1-aaa_project1/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(tX_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "#Because we have split our data into three subsets we get 3 different weight vectors with different dimensions. \n",
    "#We therefore have to create the same split for our test set to be able to multiply it by the weight vectors\n",
    "jet_0_test,indices_0,jet_1_test,indices_1,jet_2_3_test,indices_2_3 = split_test_set(tX_test,best_degree_0,best_degree_1,best_degree_2_3)\n",
    "\n",
    "y_pred_jet_0 = predict_labels(w_jet_0,jet_0_test).reshape((len(y_pred_jet_0),1))\n",
    "y_pred_jet_1 = predict_labels(w_jet_1,jet_1_test).reshape((len(y_pred_jet_1),1))\n",
    "y_pred_jet_2_3 = predict_labels(w_jet_2_3,jet_2_3_test).reshape((len(y_pred_jet_2_3),1))\n",
    "\n",
    "y_pred = np.zeros((tX_test.shape[0],1))\n",
    "\n",
    "indices_0 = indices_0.reshape(-1,)\n",
    "indices_1 = indices_1.reshape(-1,)\n",
    "indices_2_3 = indices_2_3.reshape(-1,)\n",
    "\n",
    "y_pred[indices_0] = y_pred_jet_0\n",
    "y_pred[indices_1] = y_pred_jet_1\n",
    "y_pred[indices_2_3] = y_pred_jet_2_3\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/adeye/Documents/EPFL/MA1/ML_course/projects/ml-project-1-aaa_project1/output.csv' \n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
