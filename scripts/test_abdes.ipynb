{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers.least_squares import *\n",
    "from helpers.gradient_descent import * \n",
    "import pandas as pd\n",
    "from helpers.costs import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers.proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/Users/mac/Documents/GitHub/ml-project-1-aaa_project1/data/train.csv'\n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the data into three sets, depending on whether their PRI_jet_num is {0,1,(2|3)} and return the created sets => also return the corresponding y's\n",
    "def groupy_by_jet_num(x,y):\n",
    "    #create masks to extract each one of the subsets\n",
    "    mask0 = x[:,22] == 0\n",
    "    mask1 = x[:,22] == 1\n",
    "    mask2 = x[:,22] == 2\n",
    "    mask3 = x[:,22] == 3\n",
    "    mask2_3 = np.logical_or(mask2,mask3)\n",
    "    \n",
    "    #extract the elements from each subset and return the subsets\n",
    "    jet_0 = x[mask0,:]\n",
    "    jet_1 = x[mask1,:]\n",
    "    jet_2_3 = x[mask2_3,:]\n",
    "    \n",
    "    #extract the corresponding labels\n",
    "    label_0 = y[mask0]\n",
    "    label_1 = y[mask1]\n",
    "    label_2_3 =  y[mask2_3]\n",
    "    return jet_0, label_0, jet_1, label_1, jet_2_3, label_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each one of the three sets, filter out the columns (features) that have only invalid (-999) values\n",
    "def remove_invalid_features(jet_0,jet_1,jet_2_3):\n",
    "    #we create a mask of the columns that are invalid for each subset\n",
    "    invalid_jet_1 = [4,5,6,12,22,23,24,25,26,27,28,29]\n",
    "    invalid_jet_2 = [4,5,6,12,22,26,27,28]\n",
    "    \n",
    "    #we remove the invalid elements from each subset\n",
    "    jet_0 = np.delete(jet_0,invalid_jet_1,axis=1)\n",
    "    jet_1 = np.delete(jet_1,invalid_jet_2,axis=1)\n",
    "\n",
    "    return jet_0,jet_1,jet_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(x):\n",
    "    '''go through every column and calculate it's mean'''\n",
    "    nbColumns = x.shape[1]\n",
    "    for i in range(nbColumns):\n",
    "        #we calculate the median of the current column after discarding the -999 values (they should not be in the median)\n",
    "        median = np.median(x[:,i][x[:,i]!= -999])\n",
    "        \n",
    "        #we find the indices of the elements with value -999 in our current column\n",
    "        indices = x[:,i] == -999\n",
    "        \n",
    "        #we replace the element at the found indices by the median of the current column\n",
    "        x[:,i][indices] = median\n",
    "    \n",
    "    #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient\"\"\"\n",
    "    error = y - tx@w\n",
    "    return (tx.T@error)/(y.shape[0])\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Computes the loss by MSE\"\"\"\n",
    "    error = y - tx@w\n",
    "    N= y.shape[0]\n",
    "    return 1/(2) * np.mean(error**2)\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Performs the gradient descents algorithm and returns the last weights and loss value\"\"\"\n",
    "    # initialize the weights\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # compute loss and gradient\n",
    "        loss=compute_loss(y, tx, w)\n",
    "        \n",
    "        gradient=compute_gradient(y, tx, w)\n",
    "        print(n_iter)\n",
    "        print(w)\n",
    "        #print(loss)\n",
    "        \n",
    "        # update the weights \n",
    "        w = w - gamma * gradient\n",
    "        #if n_iter % 200 == 0:\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squared_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # w use mse\n",
    "    # in the theoretical part of the session we have derived that L(w) = 1/2N e'*e\n",
    "    N = y.shape[0]\n",
    "    e = y- np.dot(tx,w)\n",
    "    \n",
    "    return (1/2) * np.mean(e**2) \n",
    "\n",
    "def compute_log_likelihood(y,tx,w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    product = np.dot(tx,w)\n",
    "    exp = np.exp(product)\n",
    "    log = np.log(exp+1)\n",
    "    one = y * product\n",
    "    diff = log - one\n",
    "    return sum(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - np.dot(tx,w)\n",
    "    N = y.shape[0]\n",
    "    return (-1/N)* np.dot(np.transpose(tx),e)\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        \n",
    "        w = w - gamma * grad \n",
    "     \n",
    "        # store w and loss\n",
    "\n",
    "    loss = compute_squared_loss(y,tx,w)\n",
    "  \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "        e = y - np.dot(tx,w)\n",
    "        N = y.shape[0]\n",
    "        return (-1/N)* np.dot(np.transpose(tx),e)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size = 1 ):\n",
    "        w = initial_w\n",
    "        for n_iter in range(max_iters):\n",
    "            for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "                grad = compute_stoch_gradient(minibatch_y,minibatch_tx,w)\n",
    "                \n",
    "                w = w - gamma * grad\n",
    "                \n",
    "        loss = compute_squared_loss(minibatch_y,minibatch_tx,w)        \n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "        e = y - np.dot(tx,w)\n",
    "        N = y.shape[0]\n",
    "        return (-1/N)* np.dot(np.transpose(tx),e)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent( y, tx, initial_w, max_iters, gamma, batch_size = 1 ):\n",
    "        w = initial_w\n",
    "\n",
    "        \n",
    "        for n_iter in range(max_iters):\n",
    "            for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "                grad = compute_stoch_gradient(minibatch_y,minibatch_tx,w)\n",
    "                print(grad )\n",
    "                w = w - gamma * grad\n",
    "                print(w)\n",
    "        #loss = compute_squared_loss(minibatch_y,minibatch_tx,w)    \n",
    "        loss = compute_loss_mse(minibatch_y,minibatch_tx,w)    \n",
    "\n",
    "        \n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    x_t = np.transpose(tx)\n",
    "    N = tx.shape[0]\n",
    "    K = tx.shape[1]\n",
    "    Id = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    one = np.dot(x_t,tx) + Id\n",
    "    two = np.dot(x_t,y)\n",
    "    w = np.linalg.solve(one,two)\n",
    "    return w, compute_squared_loss(y,tx,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \n",
    "    if (np.all(t)<-100):\n",
    "        t=-100\n",
    "    if (np.all(t)>100):\n",
    "        t=100\n",
    "    val=1/(1 + np.exp(-t))\n",
    "    print(np.all(val)>0)\n",
    "    print(val)\n",
    "    return val\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    prod = np.dot(tx,w)\n",
    "    x_t = np.transpose(tx)\n",
    "    return np.dot(x_t, sigmoid(prod)-y)\n",
    "\n",
    "def loss_logistic(data, labels, w):\n",
    "    print(data.shape)\n",
    "    print(w.shape)\n",
    "    return -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))\n",
    "\n",
    "def compute_loss_log_reg(tx,y,w):\n",
    "    print(tx.shape)\n",
    "    print(y.shape)\n",
    "    print(w.shape)\n",
    "    one = np.log(sigmoid(tx@w))\n",
    "    two = np.log(1-sigmoid(tx@w))\n",
    "    print(one, two)\n",
    "    print(\"after\")\n",
    "    return - np.sum(y*np.log(sigmoid(tx@w))+(1-y)*np.log(1-sigmoid(tx@w)))\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    print('Start')\n",
    "    loss = compute_loss_log_reg(tx, y,w)\n",
    "    print('Loss')\n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    print('Grad')\n",
    "    w = w - gamma*grad\n",
    "  \n",
    "    return loss, w\n",
    "\n",
    "\n",
    "def logistic_reg(y, tx,initial_w,max_iter,gamma):\n",
    "    \n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tx]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    w= initial_w\n",
    "    loss = 0\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        print(iter)\n",
    "\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des fonctions and stufffff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX,_,_= standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "[ 0.21438946 -0.25020933 -0.18675247  0.13013038  0.01426429  0.23598701\n",
      "  0.0102705   0.19477731  0.08424929 -0.38594631  0.19345338  0.20914762\n",
      "  0.01846474  0.20884206  0.20514039  0.20509995 -0.02520601  0.20515137\n",
      "  0.20523347  0.07841806  0.20555975 -0.56623307  0.1942683  -0.02785991\n",
      "  0.07418741  0.07427074 -0.16647721  0.01935423  0.01915115 -0.15891145]\n",
      "4.185785e-01\n"
     ]
    }
   ],
   "source": [
    "#LS with Gradient Descent Part.\n",
    "#jet_0,label_0,jet_1,label_1,jet_2_3, label_2_3= groupy_by_jet_num(tX,y)\n",
    "    \n",
    "#remove_outliers(tX)\n",
    "init_weights = np.ones((30,))\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "\n",
    "weights,loss =  gradient_descent(y, tX,init_weights , 1500, .01)\n",
    "\n",
    "print(weights)\n",
    "print(\"{:e}\".format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33944681722221465\n"
     ]
    }
   ],
   "source": [
    "#Least Squares\n",
    "weight,loss= least_squares(y,tX)\n",
    "print(compute_loss(y,tX,weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LS with StochaGradient Descent Part.\n",
    "\n",
    "remove_outliers(tX)\n",
    "init_weights = np.ones((30,))\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "\n",
    "weights,loss =  stochastic_gradient_descent(y, tX,init_weights , 500, 0.01, batch_size=1)\n",
    "\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ridge_Reg\n",
    "#print(y.shape)\n",
    "#print(tX.shape)\n",
    "\n",
    "weights, loss = ridge(y, tX, 0.05)\n",
    "print(\"{:e}\".format(np.sqrt(2*loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 30)\n",
      "Standardisé\n",
      "Start\n",
      "(250000, 31)\n",
      "(250000,)\n",
      "(31, 1)\n",
      "True\n",
      "[[0.59114628]\n",
      " [0.38927518]\n",
      " [0.34088096]\n",
      " ...\n",
      " [0.38857514]\n",
      " [0.3395192 ]\n",
      " [0.28953912]]\n",
      "True\n",
      "[[0.59114628]\n",
      " [0.38927518]\n",
      " [0.34088096]\n",
      " ...\n",
      " [0.38857514]\n",
      " [0.3395192 ]\n",
      " [0.28953912]]\n",
      "[[-0.52569177]\n",
      " [-0.94346878]\n",
      " [-1.07622194]\n",
      " ...\n",
      " [-0.94526871]\n",
      " [-1.08022478]\n",
      " [-1.23946486]] [[-0.89439785]\n",
      " [-0.4931088 ]\n",
      " [-0.41685113]\n",
      " ...\n",
      " [-0.49196322]\n",
      " [-0.41478723]\n",
      " [-0.34184139]]\n",
      "after\n",
      "True\n",
      "[[0.59114628]\n",
      " [0.38927518]\n",
      " [0.34088096]\n",
      " ...\n",
      " [0.38857514]\n",
      " [0.3395192 ]\n",
      " [0.28953912]]\n"
     ]
    }
   ],
   "source": [
    "init_weights = np.random.normal(0., 0.1, [tX.shape[1]+1,1])\n",
    "print(tX.shape)\n",
    "tX,_,_=standardize(tX)\n",
    "print(tX.shape)\n",
    "print('Standardisé')\n",
    "weight, loss= logistic_reg(y, tX, init_weights, 2, 0.01)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/abdeslamguessous/Documents/MA1/ML/ML_course/projects/ml-project-1-aaa_project1/data/test.csv' # TODO: download train data and supply path here \n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "                                     \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/abdeslamguessous/Documents/MA1/ML/ML_course/projects/ml-project-1-aaa_project1/data/output_files/output.csv' \n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def success_rate(predicted_labels,true_labels):\n",
    "    \"\"\"calculate the success rate of our predictions \"\"\"\n",
    "    success_rate = 1 - (np.count_nonzero(predicted_labels - true_labels)/len(predicted_labels))\n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-success_rate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
